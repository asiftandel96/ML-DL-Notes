{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is Deep Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what deep learning is, we first need to understand the relationship deep learning has with machine learning, neural networks, and artificial intelligence.\n",
    "\n",
    "\n",
    "The best way to think of this relationship is to visualize them as concentric circles:\n",
    "\n",
    "![DL](https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/assets/dpln_0101.png)\n",
    "\n",
    "\n",
    "At the outer most ring you have artificial intelligence (using computers to reason). One layer inside of that is machine learning. With artificial neural networks and deep learning at the center.\n",
    "\n",
    "\n",
    "Broadly speaking, deep learning is a more approachable name for an artificial neural network. The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow.\n",
    "\n",
    "\n",
    "Neural networks are inspired by the structure of the cerebral cortex. At the basic level is the perceptron, the mathematical representation of a biological neuron. Like in the cerebral cortex, there can be several layers of interconnected perceptrons.\n",
    "\n",
    "\n",
    "The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input to each node in the next layer. There are generally no connections between nodes in the same layer and the last layer produces the outputs.\n",
    "\n",
    "\n",
    "We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and are only activated by nodes in the previous layer.\n",
    "\n",
    "![layer](http://neuralnetworksanddeeplearning.com/images/tikz11.png)\n",
    "\n",
    "\n",
    "Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data.\n",
    "\n",
    "\n",
    "Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type of machine learning.\n",
    "\n",
    "\n",
    "Machine learning involves computer intelligence that doesn’t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly. Machine learning typical requires a sophisticated education, spanning software engineering and computer science to statistical methods and linear algebra.\n",
    "\n",
    "\n",
    "There are two broad classes of machine learning methods:\n",
    "\n",
    "   * Supervised learning\n",
    "   * Unsupervised learning\n",
    "\n",
    "\n",
    "In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.\n",
    "\n",
    "For example, let’s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data – i.e. employee bonus amount and tenure – we could use supervised machine learning.\n",
    "\n",
    "With unsupervised learning, there aren’t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It’s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon’s “customers who also bought…” recommendations are a type of associative task.\n",
    "\n",
    "While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique.\n",
    "\n",
    "## Why is Deep Learning Important?\n",
    "\n",
    "![importance](https://image.slidesharecdn.com/andrew-ng-extract-oct2015-nonotes-151124104249-lva1-app6891/95/andrew-ng-chief-scientist-at-baidu-30-638.jpg?cb=1448361887)\n",
    "\n",
    "Computers have long had techniques for recognizing features inside of images. The results weren’t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks.\n",
    "\n",
    "\n",
    "Facebook has had great success with identifying faces in photographs by using deep learning. It’s not just a marginal improvement, but a game changer: “Asked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.”\n",
    "\n",
    "\n",
    "Speech recognition is a another area that’s felt deep learning’s impact. Spoken languages are so vast and ambiguous. Baidu – one of the leading search engines of China – has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.\n",
    "\n",
    "\n",
    "What is particularly fascinating, is that generalizing the two languages didn’t require much additional design effort: “Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,” Andrew Ng says, chief scientist at Baidu. “The learning algorithms are now so general that you can just learn.”\n",
    "\n",
    "Google is now using deep learning to manage the energy at the company’s data centers. They’ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.\n",
    "\n",
    "## Deep Learning Microservices\n",
    "\n",
    "Here’s a quick overview of some deep learning use cases and microservices.\n",
    "\n",
    "Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what’s in the image. DeepFilter is a style transfer service for applying artistic filters to images.\n",
    "\n",
    "The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google’s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image.\n",
    "\n",
    "## Open Source Deep Learning Frameworks\n",
    "\n",
    "Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here’s an overview of each:\n",
    "\n",
    "DL4J:\n",
    "\n",
    "   * JVM-based\n",
    "   * Distrubted\n",
    "   * Integrates with Hadoop and Spark\n",
    "   \n",
    "   \n",
    "Theano:\n",
    "\n",
    "   * Very popular in Academia\n",
    "   * Fairly low level\n",
    "   * Interfaced with via Python and Numpy\n",
    "\n",
    "\n",
    "Torch:\n",
    "\n",
    "   * Lua based\n",
    "   * In house versions used by Facebook and Twitter\n",
    "   * Contains pretrained models\n",
    "\n",
    "\n",
    "TensorFlow:\n",
    "\n",
    "   * Google written successor to Theano\n",
    "   * Interfaced with via Python and Numpy\n",
    "   * Highly parallel\n",
    "   * Can be somewhat slow for certain problem sets\n",
    "\n",
    "\n",
    "\n",
    "Caffe:\n",
    "\n",
    "   * Not general purpose. Focuses on machine-vision problems\n",
    "   * Implemented in C++ and is very fast\n",
    "   * Not easily extensible\n",
    "   * Has a Python interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch and Pitts Neuron\n",
    "\n",
    "In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:\n",
    "\n",
    "1. A set of **weights** $w_i$ corresponding to synapses (inputs)\n",
    "2. An **adder** for summing input signals; analogous to cell membrane that collects charge\n",
    "3. An **activation function** for determining when the neuron fires, based on accumulated input\n",
    "\n",
    "The neuron model is shown schematically below. On the left are input nodes $\\{x_i\\}$, usually expressed as a vector. The strength with which the inputs are able to deliver the signal along the synapse is determined by their corresponding weights $\\{w_i\\}$. The adder then sums the inputs from all the synapses:\n",
    "\n",
    "$$h = \\sum_i w_i x_i$$\n",
    "\n",
    "The parameter $\\theta$ determines whether or not the neuron fires given a weighted input of $h$. If it fires, it returns a value $y=1$, otherwise $y=0$. For example, a simple **activation function** is using $\\theta$ as a simple fixed threshold:\n",
    "\n",
    "$$y = g(h) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt \\theta \\\\\n",
    "0, \\text{if } h \\le \\theta\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "this activation function may take any of several forms, such as a logistic function.\n",
    "\n",
    "![neuron](http://d.pr/i/9AMK+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a **network** can they perform useful work.\n",
    "\n",
    "Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network.\n",
    "\n",
    "Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently.\n",
    "\n",
    "The number of inputs and outputs are determined by the data. Weights are stored as a `N x K` matrix, with N observations and K neurons, with $w_{ij}$ specifying the weight on the *i*th observation on the *j*th neuron.\n",
    "\n",
    "![perceptron](http://d.pr/i/4IWA+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the perceptron for statistical learning, we compare the outputs $y_j$ from each neuron to the obervation target $t_j$, and adjust the input weights when they do not correspond (*e.g.* if a neuron fires when it should not have).\n",
    "\n",
    "$$t_j - y_j$$\n",
    "\n",
    "We use this difference to update the weight $w_{ij}$, based on the input and a desired **learning rate**. This results in an update rule:\n",
    "\n",
    "$$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "After an incremental improvement, the perceptron is shown the training data again, resulting in another update. This is repeated until the performance no longer improves. Having a learning rate less than one results in a more stable learning rate, though this stability is traded off against having to expose the network to the data multiple times. Typical learning rates are in the 0.1-0.4 range.\n",
    "\n",
    "An additional input node is typically added to the perceptron model, which is a constant value (usually -1, 0, or 1) that acts analogously to an intercept in a regression model. This establishes a baseline input for the case when all inputs are zero.\n",
    "\n",
    "![bias](http://d.pr/i/105b5+)\n",
    "\n",
    "## Learning with Perceptrons\n",
    "\n",
    "1. Initialize weights $w_{ij}$ to small, random numbers.\n",
    "2. For each t in T iterations\n",
    "    * compute activation for each neuron *j* connected to each input vector *i*\n",
    "    $$y_j = g\\left( h=\\sum_i w_{ij} x_i \\right) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt 0 \\\\\n",
    "0, \\text{if } h \\le 0\n",
    "\\end{array} \\right.$$\n",
    "    * update weights\n",
    "    $$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "\n",
    "This algorithm is $\\mathcal{O}(Tmn)$\n",
    "\n",
    "### Example: Logical functions\n",
    "\n",
    "Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables `x1` and `x2`, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from scipy import optimize\n",
    "from ipywidgets import *\n",
    "from IPython.display import SVG\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  0\n",
       "2   1   0  0\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)})\n",
    "AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize weights to small, random values (can be positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.15975675e-05, -4.00681619e-05,  2.00060227e-04])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a simple activation function for calculating $g(h)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda inputs, weights: np.where(np.dot(inputs, weights)>0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(inputs, weights)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a training function that iterates the learning algorithm, returning the adapted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets, weights, eta, n_iterations):\n",
    "\n",
    "    # Add the inputs that match the bias node\n",
    "    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n",
    "\n",
    "    for n in range(n_iterations):\n",
    "\n",
    "        activations = g(inputs, weights);\n",
    "        weights -= eta*np.dot(np.transpose(inputs), activations - targets)\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it first on the AND function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = AND[['x1','x2']]\n",
    "target = AND['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it has learned the function perfectly. Now for OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\n",
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = OR[['x1','x2']]\n",
    "target = OR['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also 100% correct.\n",
    "\n",
    "### Exercise: XOR\n",
    "\n",
    "Now try running the model on the XOR function, where a one is returned for either `x1` or `x2` being true, but *not* both. What happens here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the problem graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAADxCAYAAAAp+dtsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAu9klEQVR4nO3de1xUdf7H8ddwHxS84DCplJdMyYR0t91cfqWbpZiBolgiKGVGXkLUNVMTw9YQb4WXtBRdr6CYUmhtSGm1tVquXdQ0MzN1Me7kBQUZmPn94UoiIDMww5kZPs/HYx5xOGfmvAfIz3wv53tUBoPBgBBCiCbNQekAQgghlCfFQAghhBQDIYQQUgyEEEIgxUAIIQRSDIQQQiDFQAghFFVcXExQUBBZWVnV9v3www+EhoYSGBjI7NmzKS8vt1gOKQZCCKGQw4cPM3LkSM6cOVPj/unTpzNnzhz27NmDwWBg+/btFssixUAIIczs0qVLZGVlVXtcunSpynHbt28nLi4Ob2/vaq9x/vx5SktL6dmzJwDDhg0jIyPDYpmdLPbKQghhR4oooTVqo451cXFh2LBhXLx4scr3o6OjmTRpUuV2fHx8ra+Rl5eHRqOp3NZoNOTm5pqY2ng2XQx+++0Ker1xq2l4eTWnsLDYwomMI1lqJlmsNwfYbhYHBxWtWjVr8Dlbo+Yh/kEWl257nA+efOH2LOnp6VRUVFTZ5+npafT5alopSKVSGf18U9l0MdDrDUYXgxvHWwvJUjPJUp215ADJkmW4xFku1n2gCtq2bdugc2m1WgoKCiq38/Pza+xOMhcZMxBCCGPpVcY9zKB9+/a4urry9ddfA/Dee+/Rp08fs7x2TaQYCCGEsQwq4x4NEBUVxdGjRwFYsmQJCQkJPP7445SUlBAZGWmOd1Ejm+4mEkKIRmWhnql9+/ZVfp2UlFT5ta+vLzt27LDMSW8hxUAIIYxlUAF1ffK33CCvJUkxEEIIY1nP+LnZSTEQQgijGTEmYMHpn5bUZIpBTk4OOp0jbm5uSkcRQtgqPXW3DmyzFjSN2UR6vZ6xY8cSGTmCs2fPKB1HCGGrGmE2kVKaRDFwcHBg2rRp5ObmEB4eSmam5db3EELYMSkGtq9fv35s2/Yud9/dhZdemsKCBfMoKytTOpYQwpYYjHzYoCZTDADatm3HunWbGT36GQ4dOkhFheXWBhdC2CE7bhk0mQHkG5ydXZg2bSYlJSWo1WquXr3Cd999Q0DAw0pHE0JYOwN1Lzdhox+xbTR2w6nV15ei3bBhHRMnRvHGG4vQ6XQKpxJCWDU77iZqci2DW40dO47ffvuNTZv+weHD37JoUSJa7R1KxxJCWCNjuoFstJuoybYMbnB1dWX27DgWLHidn376kREjQvj226+VjiWEsEZ23DJo8sXghoEDnyAlZQd3390Fb2+t0nGEENbIjgeQpRjcpGPHzqxbt4X27X0wGAysXr2SgoJ8pWMJIayFtAzqp7i4mKCgILKysmo95tNPP6Vfv36WjFEvP//8E+vXJzFixFD+858vlY4jhLAGjXhzm8ZmsWJw+PBhRo4cyZkzZ2o9pqCggIULF1oqQoN06dKVzZtT8fDwYNy4Z0lKehu9Xq90LCGEoozpIpJiUMX27duJi4u77T07Y2NjiY6OtlSEBrvnnm4kJ7/DgAGPs3LlUl55ZZbSkYQQSrLjbiKLTS2Nj4+/7f5NmzbRvXt37r///nqfw8uruUnHazQeJp9Do/Fg7drVbNmyhU6dOtXrNcyVxVIkS82sJYu15ADJYs9TSxW5zuDkyZNkZmayYcMGcnJy6v06hYXF6PXGlWGNxoP8/Mv1PtfAgSEA5OdfZvXqlajVakaPHoOqHmuXNzSLOUmWmllLFmvJAbabxcFBZfIHx1oZ88nfRlsGiswmysjIID8/n9DQUJ5//nny8vIIDw9XIorJ9Ho9P/10kjfeWMTUqdFcunRR6UhCiMYiA8jmFRMTw549e0hPT2fNmjV4e3uTkpKiRBSTOTg4sHjxUqZPf5kvvviMsLBhHDt2VOlYQojGINcZmEdUVBRHj9r+P5wqlYqIiEj+8Y8t6PV6oqKe5sKF35SOJYSwNBlArr99+/ZVfp2UlFRtv4+PT5VjbIm/f0+2bUvj8OFvadmyFQA6XRnOzi4KJxNCWIyNfvKvi1yB3EAtW7aib9/rF819/PEehg8fzI8/nlA4lRDCIuy4ZSDFwIxat/bi6tUrREaO4N13d2Aw2OhfhRCiZjJmIIzxhz88QGrqe/Ts+UdefTWWOXNmUlJyVelYQghzkdlEwlitW3uxalUS48dH88EHu/j8838pHUkIYS523E3U5G9uYwmOjo6MHx/NY48F0qXLPQCcP59F+/Y+CicTQjSIHV+BLC0DC7pRCM6e/YXQ0CD+/vc5lJaWKpxKCFFvdtwykGLQCNq3v5Pw8EjS0t7h6adH8ssvvygdSQhRX3ZYCECKQaNwcnIiJuZvLF/+NtnZvzJw4EA++ihD6VhCCFPJbCJhDn36/JXU1He55557+O67b5WOI4QwlR3PJpIB5EbWtm070tLSKCi4vuLiiRPH8fDwlMFlIWyBDCALc3JxccHZ2QWDwcArr8wiLGwYn31mm0tyCNGkyACysASVSsXrr6+gfXsfJk+eSGLiYnQ6ndKxhBC1kTEDYSl33nkXGzdu5cknw9i4cR1RUU/LPRKEsFYWaBns3r2bQYMG0b9/f5KTk6vtP3bsGKGhoQwePJhx48Zx6dKlhr2HWkgxsAKurq7Mnj2XhIQltGrVmmbNzHRXJiGEeRmoe/DYhGKQm5tLYmIiKSkppKenk5qayqlTp6ocEx8fT0xMDLt27aJTp06sW7fOvO/pf6QYWJHHHw8iMfFNHB0dyc/PY9261VRUVCgdSwhxgwndRNnZ2WRlZVV53Pqpfv/+/fTu3ZuWLVvi7u5OYGAgGRlVp53r9XquXLkCQElJCW5ubhZ5azKbyEp9+OH7rFiRyFdfHSAhYQleXm2UjiSEMOEeyBEREZw/f77KrujoaCZNmlS5nZeXh0ajqdz29vbmyJEjVZ4zc+ZMxowZw/z581Gr1Wzfvr0h76BWUgysVGTks3h6tmDBgnmMGDGUhIQl/OlPDyodS4imzYSppcnJydVa9p6enlUPrWGZe5Xq99cvLS1l9uzZbNy4EX9/f9avX8+MGTNYs2ZNPd9A7aSbyIqFhISyeXMqzZs3Z9y4Mezdm6l0JCGaNhMGkNu2bYuPj0+Vx63FQKvVUlBQULmdl5eHt7d35fbJkydxdXXF398fgBEjRnDw4EGLvDWLF4Pi4mKCgoLIysqqtu/jjz9myJAhDB48mIkTJ3LxosyiudU993QjOfkdRo4czZ/+1FvpOEI0bWaeWhoQEMCBAwcoKiqipKSEzMxM+vTpU7m/Q4cO5OTkcPr0aQD27t2Ln5+f2d8WWLgYHD58mJEjR3LmzJlq+4qLi5k7dy5r1qxh165ddOvWjRUrVlgyjs1q1qw506fPwtPTk7KyMqZOjea7775ROpYQTY/BiKUoTCgGWq2WqVOnEhkZSUhICEFBQfj7+xMVFcXRo0dp0aIFCQkJTJkyheDgYHbu3Mn8+fMt8tYsOmawfft24uLieOmll6rt0+l0zJ07F61WC0C3bt3YvXu3JePYhby8XH766Ueeey6SyZOnMWrUM1X6GIUQFmTCALKxgoODCQ4OrvK9pKSkyq/79u1L3759TXvRelAZGuFGvf369WPTpk34+NS8/k5paSnh4eGMHj2aoUOHWjqOzbt48SLTpk3jww8/JDAwkMTERFq0aKF0LCHsXsevt3P2WvFtj+ng2pwzf3yqkRKZj+KziS5fvszEiRPx9fU1uRAUFhaj1xtXyzQaD/LzL9cnotk1PIsD8+e/wX339WTp0sWMHz+R5cvfViiL+UgW680BtpvFwUGFl5eZLuS0QMvAWihaDPLy8hg7diy9e/fm5ZdfVjKKzVGpVIwa9TT+/vfTvLkHADpdGU5OztJtJISl2PGqpYoVg4qKCsaPH8/jjz/OxIkTlYph8/z9ewLX5yvHxc2moqKCV175uyxpIYQl2HHLoNGvM7gxSr5v3z6OHz/Onj17GDJkCEOGDGH27NmNHceudOlyDx99lEF4+HBOnvxR6ThC2B+5uU3D7Nv3+1r9N0bJ/fz8OHHiRGOcvklQqVQ8++zz+Pv3ZNasaYwe/RSzZr3CkCHDpNtICHOy0W6gusgVyHbmgQf+zLZt79Kz5x94/fWF/Pbbb0pHEsJ+2PHNbRSfTSTMz8urDatWreXs2TO0bt0ag8FAdvavtGvXXuloQtg2Ox5AlpaBnXJ0dKRz57sB2Lp1M6Ghwfzzn3JRnxANYsctAykGTcCjjwbi63svL788nddei+PatWtKRxLCNtnxALIUgyZAq9WSlLSRMWOi2LEjlcjIMM6dO6t0LCFsj9wDWdg6JycnJk+exvLlb5GXl0NOTrbSkYSwPXbcTSQDyE1Mnz6P8MEHH+Pu3gy4Pu3X1/d+nJ1dFE4mhI2w0X/s6yItgyboRiE4c+Y0kZGRjBkzil9/PV/Hs4QQ0k0k7FLHjp1ZvXo1Z86cJixsGP/61ydKRxLCutlxN5EUgybuiSeeICVlJ23btiMmZgIrVy5TOpIQ1kvvYNzDBtlmamFWd93VgU2btjF8+AhatWqldBwhrJcdtwxkAFkA4OrqSmzsq9y419Fnn+3DxcWVv/zl/xROJoQVkSuQRVOhUqkwGAysX7+WiROfY9Wq5VRUVCgdSwjrYMctAykGohqVSsVbb60lKGgIa9asYuLE5ygsLFA6lhDWwQ5nEoEUA1ELtdqdefMWMHduPN999w1hYUMpKipUOpYQyrLjloGMGYjbCgkJpXv3Huzdm0nr1l5KxxFCWcasPSRrE1mf8nLYsMGZgAB3WreGoCA1+/Y5Kh3L5nTt2o0JEyYBcOrUSV58MYYLF+Q+CZZyjXJWqQ/xp9br8GIRw1vs4EvnLKVjCZCLzhqiuLiYoKAgsrKq/zH/8MMPhIaGEhgYyOzZsykvLzfbeQ0GGDPGjbg4V06dcuS33+DgQSfGjFGTlORstvM0NadO/cRnn31CWNgwDh/+Vuk4dkdHBaEtd7Cw2X7OOl6kiBL+5XyOES3SSHOVOwMqzo67iSxaDA4fPszIkSM5c+ZMjfunT5/OnDlz2LNnDwaDge3bt5vt3J995sjnnztRUlK1SpeUqJg3z5VLl8x2qiZl4MAn2LhxK46OjowdO5rNmzdUTkcVDbfb9Se+d8ynRHXTByMVlKjKebH5x1zDfB+YRD1Iy6B+tm/fTlxcHN7e3tX2nT9/ntLSUnr27AnAsGHDyMjIMNu5U1OduXq15n1OTvDRRzJcUl/du/dg69Y0Hn74r7z++gLS09OUjmQ3Uty+56qDrsZ9KmC/dBcpy45bBhb9FzE+Pr7WfXl5eWg0msptjUZDbm6uSa/v5dW81n23mxqv16twdlZz0+kbnUbjodzJb1GfLBqNB5s3b2Dnzp0MHjwYFxcXdDodzs4N64Kz9Z9LQ+nQ17pP5aDCtaUzGpT7GTX13w8GIwaQbbRloNjH45q6FlQq036IhYXF6PU1l+G//tWJzEw3rl6t/pp6vQE/vyvk5ytTwjUaD/LzLyty7ls1NEvfvoFcvHiNixdziYwMIzw8kqeeGmny79IcWcxJqSyPqjvyrXsOpQ7Vu4OuGcrpWtiKfIMyPyNb/f04OKhu+8HRJHIFsvlptVoKCn6/kCk/P7/G7qT6Cgkpp1UrA05OVf/Bd3MzMGBAOZ062WhbzkoZDAZ8fO4iIeHvzJo1jStXipWOZJNGl/rRDGccbvkHRW1wIqKkB20M7golE4BddxMpVgzat2+Pq6srX3/9NQDvvfceffr0Mdvru7tDRsZVHn64AhcXAx4eoFYbGDVKx6pVpWY7j7iuZctWrFjxNpMmTSUzM4Pw8OH89NOPSseyOa0Maj78bSR/1rXDxeBIc1xopndm/NU/En/lEaXjCTseQG70bqKoqChiYmLw8/NjyZIlxMbGcuXKFbp3705kZKRZz6XVGkhNLaGoCMADN7di3OWDlcU4ODgwduw47r+/FzNnTmPFikSWL39b6Vg2p6O+JbsujiBfdRXHNo64FzriJteHWgdjPvnbaMugUf7C9u3bV/l1UlJS5de+vr7s2LHD4udv3Ro0GsjPt/ipBPDAA38mNfXdynGDoqIi1Go1arVa4WS2RWNwR4MH+VhHP71AxgyEMJWXVxtat/bCYDAwY8ZURo9+il9+Oa10LCEaxsDvS1LU9rDRloEUA2FRKpWKMWOiKCwsIDx8OP/8526lIwlRfzKALET9BQQ8xLZt7+Hrey8vvzyd+Pi5XLt2TelYQpjOAgPIu3fvZtCgQfTv35/k5ORq+0+fPs3o0aMZPHgwY8eO5eLFi+Z6N1VIMRCNQqvVsmbNBp555jkOHPi3FANhm8zcMsjNzSUxMZGUlBTS09NJTU3l1KlTv5/OYGDChAlERUWxa9cu7r33XtasWWO+93MTmaIgGo2zszNTprzI889PwN29GWVlZRw6dJCAgIeUjiaEcUwYQM7Ozq52l0BPT088PT0rt/fv30/v3r1p2bIlAIGBgWRkZBAdHQ3AsWPHcHd3r5x2P378eC5ZaGE1KQai0bm7NwMgJWUzS5cuJiLiaaZMmaZwKiGMYML9DCIiIjh//nyVXdHR0UyaNKly+9Zleby9vTly5Ejl9rlz52jTpg0zZszg+PHjdO3alTlz5pjhjVQnxUAoJiJiNLm5OSQnb+TIke9Yty4JFxfPup8ohFJMuM4gOTm5xpZBlUPrWJanvLycgwcPsmXLFvz8/Fi6dCkLFixgwYIF9Ul/WzJmIBTj7OzCjBmzWbx4KadPn2LAgAEcPPil0rGEuD0jxwvatm2Lj49PlcetxeDWZXny8vKqLMuj0Wjo0KEDfn5+AAQFBVVpOZiTFAOhuP79B7J1axpdu3bFy6uN0nGEqJ2ZZxMFBARw4MABioqKKCkpITMzs8qyPL169aKoqIgTJ67f2Gjfvn3cd999Zn9bIN1EwkrcdVcH0tLSKCgoxmAwsGHDOp54Ihhvb63S0YT4nZmXo9BqtUydOpXIyEh0Oh3Dhw/H39+/yrI9K1euJDY2lpKSEu644w4WLVrUkHdQK5XBhm9TdbslrG9lq8vvWpo1Zvnvf8/x5JNDUKvVJCQsoXfvAMWyKM1acoDtZjHnEtYdN33C2csltz2mg4eaM5G2t6igdBMJq3PnnXeRnPwOrVq1ZsKEsbz11opqA3FCKKKupSiMmW1kpaQYCKt0991dSE7ezhNPDGb16pXMmPE3pSMJIUtYC6EEtdqdefMW8MADf6Zly1ZKxxFClrAWQikqlYqQkNDK7S1bNqLTlfH002NxcJCGrVCAjX7yr4v83yRshsFg4Pjx71m27HViYiZw4cJvSkcSTY2sWiqE8lQqFfHxi5g16xW++mo/YWHDOHLkO6VjiabEjscMLFoM6lqa9dixY4SGhjJ48GDGjRtnsQWYhP1QqVSMGBHOhg0pODg48NxzkeTl5SodSzQVMpvIdHUtzQoQHx9PTEwMu3btolOnTqxbt85ScYSdue8+P7ZtSyM+flHlhWk6nU7hVMLuSTeR6W5emtXd3b1yadab6fV6rly5AkBJSQlubm6WiiPskKdnC/r3HwjAgQP/ZtiwIH744ZjCqYRds+NuIovNJqpraVaAmTNnMmbMGObPn49arWb79u0mncPUqwo1Gg+TjrckyVKz+mZp315DeXkZTz89kldffZXRo0dXWf2xMbOYm7XkAMkiU0vroa6lWUtLS5k9ezYbN27E39+f9evXM2PGDJPu4iPLUTScvWS5666upKSkERv7ErNmzeKzz75gzpxXadasfssQWMvPxVpygO1mMedyFGDMJ3/bbBlYrJuorqVZT548iaurK/7+/gCMGDGCgwcPWiqOaAJatWrFihWriY6eQmbmh3z00R6lIwl705QHkMvLy6t9z5gbMte1NGuHDh3Iycnh9OnTAOzdu7dyzW4h6uv6DKPxbNv2LkOGDAMgO/tXhVMJu9EUB5C///57HnnkEXr16sWUKVMoLi6u3PfMM8/U+cI3L80aEhJCUFBQ5dKsR48epUWLFiQkJDBlyhSCg4PZuXMn8+fPN8ubEqJr126oVCqys3/lyScH88orsygpuf1qk0LUqSkOIMfHxzN37lx69OhBQkICzz33HJs2bcLFxaXG8YCaBAcHExwcXOV7SUlJlV/37duXvn371jO6EHXz9tYycuRo1q59m+PHv2fx4mV06tRZ6VjCVtnxAHKtLYPS0lL69u2Ll5cXS5Yswdvbm1mzZjVmNiEazNHRkRdemMzKlWsoLCwgImI4GRkfKB1L2CorbxlMmjSJ/fv31+u5tRYDvV5PYWFh5fbChQs5deoUK1eubPCUPSEaW0DAw2zb9i5du/qyf/8XSscRtsrKxwwGDBjAqlWrCAwMZN26dVy4cMHo59baTfTss88SEhLCa6+9Rt++fVGr1bz11luMGjWK3Fy5/F/YHq32DpKSNlbeKOf06Z9xcXHBx+dOhZMJm2Gg7tlCChaDG13zP//8Mzt37uTJJ5+kZ8+ejB49unLmZm1qbRkMGTKEjRs3cuzY71d0tmvXjnfeeUdaBsJmOTs74+bmhsFgYO7c2YSFDWPv3kylYwlbYeXdRHC9V+fs2bOcOXOG8vJyvLy8mDt3LosXL77t8247tbRz5868//77xMXFUVZWRlZWFhMmTCAgoPHvSSuEOalUKhISltChQ0emTYthyZIEdLoypWMJa2fl3USJiYn07duXtWvXMmjQIDIzM5k5cyZbtmxhx44dt31unVcg79ixg/j4eEJDQ7l06RLR0dE8+eSTZgsvhFLat/dh/fpkEhMXsWXLRo4c+Y6lS9+idevWSkcT1sqYT/4KtgyKiopISkrC19e3yvfd3d15/fXXb/vcOouBSqXCxcWFkpIS9Hq9dBEJu+Li4sKMGbH06vUA7777Dh4e5lq2QNglK59aOm/evFr3PfTQQ7d9bp1XIAcHB1NcXEx6ejpbtmxh27ZtTJgwwfSUQlixAQMGsmrVWpydXbh48QJvvvlmjVffiybOBsYM6qvOYhAdHc3ixYtp1qwZHTp0YOvWrXTq1KkxsgnRqG60evfs+ZCEhATGjXtGbpwjqjIA+joe9nbR2Q0hISFVtp2dnXnppZcslUcIxT311EiWLVvGsWPHCAsbxldfHVA6krAWTbllIERTNHz4cJKT36Fly5aMH/8sH3ywW+lIwhpIMRCi6bn77i5s2bKdsLBR9O79F6XjCGtg5VNLG0KKgRC34e7ejBkzZuPl1Yby8nJmzXqRb745pHQsoRRpGQghCgsL+P77o0RFPc369WvR6/VKRxKNrSnf3EYIcZ1Wewdbt+6kX7/+LFu2hClTJnLx4gWlY4nGJN1EQgiA5s2bs2hRIjNnxrJ//7+ZNi1G6UiisdlhFxEYcQWyEKIqlUpFWNgoevS4HweH6//zl5eX4+joKFfo2zsrvwK5IaQYCFFPPXr8fs/uJUsSKCjIJy4uHg8PDwVTCYuy8rWJGsKi3US7d+9m0KBB9O/fn+Tk5Gr7T58+zejRoxk8eDBjx47l4sWLlowjhEUYDAbatWvPJ5/sJTw8lBMnjisdSViKjBmYLjc3l8TERFJSUkhPTyc1NZVTp05V7jcYDEyYMIGoqCh27drFvffey5o1aywVRwiLUalUREY+y9q1m7l27RqRkWHs2LHN6HuFCxsis4lMt3//fnr37k3Lli1xd3cnMDCQjIyMyv3Hjh3D3d2dPn36ADB+/HgiIiIsFUcIi+vV6w9s2/YuDzzwZ5YsWUhubo7SkYS5WeA6g7p6UG749NNP6devX0PfQa0sNmaQl5eHRqOp3Pb29ubIkSOV2+fOnaNNmzbMmDGD48eP07VrV+bMmWPSOby8TFtuWKOxnr5cyVIzW8+i0XiQmrqVEydO0L17VwwGA9nZ2bRr165Rc1hKk89i5gHkGz0oaWlpuLi4EBYWxoMPPkiXLl2qHFdQUMDChQtNjmsKixWDmprIN8+0KC8v5+DBg2zZsgU/Pz+WLl3KggULWLBggdHnKCwsRq837iev0XiQn3/Z6Ne2JMlSM3vKotHcSX7+ZXbtepf581/l5ZfjGDx4aKPnMCdbzeLgoDL5g2PtjPnkf31/dnZ25f22b/D09MTT07Ny++YeFKCyByU6OrrK82JjY4mOjq7zBjUNYbFuIq1WS0FBQeV2Xl4e3t7eldsajYYOHTrg53d9RkZQUFCVloMQ9iAg4CF69PDnlVdmMXfubEpKSpSOJBrChAHkiIgIHn300SqPjRs3Vnm5mnpQcnOrLpu+adMmunfvzv3332+pdwVYsGUQEBDAihUrKCoqQq1Wk5mZWeUuPL169aKoqIgTJ07g6+vLvn37uO+++ywVRwhFtGmjYfXq9bz99pskJb3FsWNHWbx4KR07dlY6mqgPE6aWJicn19gyqHJoHT0oJ0+eJDMzkw0bNpCTY9kxKIsVA61Wy9SpU4mMjESn0zF8+HD8/f2JiooiJiYGPz8/Vq5cSWxsLCUlJdxxxx0sWrTIUnGEUIyjoyMvvDCZnj3/QGzsS/zyyy9SDGyVMbOF/re/bdu2db6cVqvl0KHfFz68tQclIyOD/Px8QkND0el05OXlER4eTkpKSv3y34bKYMPz32TMoOEkS80slaW4uJjmza/3X3/11QF69fojLi4ujZ6jPmw1iznHDDq+dIyzhWW3PaaDlwtnFhnXy5Gbm8vIkSPZsWMHarWasLAw5s2bh7+/f7Vjs7KyiIyMZN++ffXKXhdZm0iIRnSjEOTkZPPCC8/zzDMjOX8+S+FUwmhmnlp6cw9KSEgIQUFBlT0oR48eteAbqU5aBgqQLDVralk++WQvr7wyC4PBwLx5C3jkkUcVyWEsW81i1pbBi8eNaxks6W6W8zUmaRkIoZBHHnmUbdvSuOuuDkyd+gKJiYuVjiSMYaerlkoxEEJB7dv7sGFDCiNHjqJZs2ZKxxF1sePlKGTVUiEU5uLiwowZsZXTDL/66gBlZWU8/HBfhZOJaux4CWtpGQhhJW7ML1+/PolJk8axfPkblJeXK5xKVCH3QBZCNJalS1cxdOiT/OMfaxgxYgT5+XlKRxI3yBLWQojG4ubmRlzcPObNW8Dhw4cZMWIo2dm/Kh1LgF23DGTMQAgrFRwcwv/935/ZuDGZO+6o+2pW0QhkzEAIoYRu3boxdep0VCoV//3vOV58MYaiokKlYzVdBiNmEtloy0CKgRA24qeffuRf//qUESOG8s03h+p+gjA/O+4mkmIghI3o168/mzal4uamJirqadavX4ter1c6VtMiA8hCCGvg63svW7fu5JFHHmPZsiWkptZ+m0RhAXbcMpABZCFsTPPmzVm8eCnvv5/OY48FAtfvHOjkJP87W5wMIAshrIlKpSI4OAS1Ws3Vq1cIDw8lJWVTjTdLEWZkx8tRSDEQwsaVl1fQtm07Fi2az/Tpk7l82TpWFrVbdthFBFIMhLB5np6eLF26iilTpvPJJ3sJDw/lxInjSseyTzKAXD+7d+9m0KBB9O/fn+Tk2ge6Pv30U/r162fJKELYNZVKxTPPjGXt2k2UlpayeHGCdBlZggwgmy43N5fExETS0tJwcXEhLCyMBx98kC5dulQ5rqCggIULF1oqhhBNSq9efyQ19V10Oh0qlYpLly7h5OSIu7ssj20WMoBsuv3799O7d29atmyJu7s7gYGBZGRkVDsuNjaW6OhoS8UQoslp3doLrfYOAOLiXmbUqKf4+edTCqeyE9IyMF1eXh4ajaZy29vbmyNHjlQ5ZtOmTXTv3p3777+/Xucw9VZ2Go1Hvc5jCZKlZpKluobkGDfuOaKjoxk16kkWLFjA8OHDFctibopkMWa2kI3OJrJYMaipv/LGeu0AJ0+eJDMzkw0bNpCTk1Ovc8g9kBtOstTMWrI0NIevb0+2bk1j5sxpTJ48mU8//ZwZM2Jxc3Nr9CzmpNQ9kKWbqB60Wi0FBQWV23l5eXh7e1duZ2RkkJ+fT2hoKM8//zx5eXmEh4dbKo4QTZZG483q1esZO3YcX3zxL65cKVY6kg0zpovINlsGFisGAQEBHDhwgKKiIkpKSsjMzKRPnz6V+2NiYtizZw/p6emsWbMGb29vUlJSLBVHiCbNycmJSZOmkpb2AV5ebSgvL+fLL/crHcv2yNRS02m1WqZOnUpkZCQhISEEBQXh7+9PVFQUR48etdRphRC34eFxvZ89Le0dxo9/lgUL5lFWVqZwKhsiA8j1ExwcTHBwcJXvJSUlVTvOx8eHffv2WTKKEOImQ4cOJyvrHJs2refIkcMsXryU9u19lI5l/WTMQAhhT5ydnfnb32bwxhtvcu7cWcLChvHvf3+udCzrJ2sTCSHsUb9+j7FtWxqdO99NixYtlY5j/ey4m0iKgRBNnI/PnWzYkEKPHn4AbN26mZycbIVTWSkZQBZC2LMb1wDl5eWyYkUiYWFDpduoNnbYKgApBkKIm3h7a0lJ2YFG480LL0Tx5ptLKS8vVzqW9ZCWgRCiqejYsTObN29n6NDhrF37NlOnviAroN5gxwPIcp88IUQ1bm5uxMW9Rq9ef8TJyanKUjJNmjFdQTbaVSTFQAhRq8GDh1Z+nZqays8/n2Xs2HE4ODTRTgW5zkAI0dQdOnSIlSuXER39PEVFRUrHUYZMLRVCNHWLFi0iNvZVDh06SFjYUL799hulIzU+GUAWQjR1KpWK4cNHsGnTNlxdXYmKiiQr679Kx2p8Zi4Edd0e+OOPP2bIkCEMHjyYiRMncvHixYakr5UUAyGESXx9u5OSspNXX52Pj8+dAE1n+qnewbiHkW7cHjglJYX09HRSU1M5der3u9IVFxczd+5c1qxZw65du+jWrRsrVqywxDuTYiCEMJ2HhwdPPDEYgMOHv2XYsCc4evRIHc+yAyZ0E2VnZ5OVlVXlcenSpSovV9ftgXU6HXPnzkWr1QLQrVs3srMtc3W4FAMhRIM4OTmh0+kYMyaCrVs32/c1CSYMIEdERPDoo49WeWzcuLHKy9V0e+Dc3NzK7VatWvHYY48BUFpaypo1ayq3zU2mlgohGuS++/zYti2NOXNmsnBhPF9/fYi5c+Np3txMt5q0JiZMLU1OTqaioqLKLk9Pz6qH1nF74BsuX77MxIkT8fX1ZejQodX2m4O0DIQQDdaiRUuWLl3F5MnT+OSTj0lPT1M6kmWY0DJo27YtPj4+VR63FoO6bg9843vh4eH4+voSHx9vsbcmxUAIYRYODg6MGRNFcvIOwsIigOsDpHbVbWTmqaV13R64oqKC8ePH8/jjjzN79myLXglu0W6i3bt389Zbb6HT6XjmmWeIiIiosv/jjz9mxYoVGAwGfHx8SEhIoEWLFpaMJISwMF/fewEoLCwgLGwoAQEPMXt2HO7uzRROZgYG6l57yIRicPPtgXU6HcOHD6+8PXBMTAw5OTkcP36ciooK9uzZA0CPHj0s0kJQGSxUtnNzcxk5ciRpaWm4uLgQFhbGG2+8QZcuXYDrU6YGDhzIzp070Wq1LFu2jMuXLxMbG2v0OQoLi9HrjYuv0XiQn3+5Xu/F3CRLzSSL9eYA07NUVFSwbt1q3nprBZ06dWbx4mXcfXeXRs/i4KDCy8s84xcdBxZw9lf9bY/p0M6BMxltzHK+xmSxbiJrmjIlhGh8jo6OPP/8RN5++x9cuHCBiIgnef/9dKVjNYxcgWw6a5oyJYRQzoMP/oXU1He57777+OSTj217DMGO1yay2JhBY0yZMrXpp9F4mHS8JUmWmkmW6qwlB9Q/i0bjQVraTq5du0azZs04d+4c5eXldO7cudGzNIgdr1pqsWKg1Wo5dOhQ5XZtU6bGjh1L7969efnll00+h4wZNJxkqZm1ZLGWHGC+LFevXmbq1GkcPXqYuLh4BgwYaNEs5hwzMOrmNTZ6cxuLdRNZ05QpIYR1iYt7jbvv7sJLL01hwYJ5lJWVKR3JSMZ0Ednmv2UWbRlYy5QpIYR1adu2HevWbWb58jfYvHkDR44cZunSlXh7a5WOdnvSTVQ/wcHBBAcHV/leUlISAH5+fpw4ccKSpxdCWDFnZxemTZtJr15/ZMuWjXh4eNb9JKXZ8W0v5QpkIYSi+vXrz7p1m1Gr1Vy9eoX165PQ6XRKx6qZTC0VQgjLuTFmuHfvRyxb9jrPPRdJbm6OwqlqYMdTS6UYCCGsRnBwCAsWvM5PP/1IWNhQ9u//XOlIVd2YTVTXwwZJMRBCWJWBA58gJWUHbdpoeOGF50lLe0fpSL+TbiIhhGg8HTt2ZtOmVMLCIujdO0DpOFXZYRcRSDEQQlgptVrNjBmxtGvXHr1ez6uvxvKf/3ypbChpGQghhHIuXPiN7777hnHjniUp6W30+tuvHGoxMoAshBDKad3ai+TkdwgMHMTmzevJy8tTJogdtwzkHshCCJvg7t6M+fMXk5OTzR133KHMmk16FdTVKLHR2URSDIQQNkOlUtG2bTvlAhhURixHIcVACCHsm6xNJIQQ4vqqpUYcY4OkGAghhLGkZSCEEEIGkIUQQsgAshBCCKSbyNZduQJlZaBSgbOz0mmEME5xMeh01/9unZrE/6k2wI5bBnZ9BXJhoYrnnnOjW7fm3HMPdOvWnIQEFyoqlE4mRO2ys1VERFz/u+3SBbp3b86bbzpjsNFPnHbFjq9Atmgx2L17N4MGDaJ///4kJydX2//DDz8QGhpKYGAgs2fPpry83GznvnYNBg1S8+GHTpSVqSgpgeJiFW+/7cKUKW5mO48Q5nTpEgwY4M6+fU7odNf/bi9cULFkiSvz5rkoHU+AXa5LBBYsBrm5uSQmJpKSkkJ6ejqpqamcOnWqyjHTp09nzpw57NmzB4PBwPbt2812/vR0J/LyHNDpqv5ySkpUvPeeE//9r+3+0oT9Sk525tIlFRUVVf8+r15VkZTkwoULyuQS/yM3tzHd/v376d27Ny1btsTd3Z3AwEAyMjIq958/f57S0lJ69uwJwLBhw6rsb6gPP3TiypWafymOjvD5545mO5cQ5vL++06UlNT8d+viAl9+KX+3irLjbiKLDUvl5eWh0Wgqt729vTly5Eit+zUaDbm5uSadw8urea37PDxqf56DgwovLzU3nb7RaTS3CdjIJEvNlMjSrFnt+1QqFRqNu/zd/o8iWYwaQG6UJGZnsWJgqGG068ZNr43Zb4zCwmL0+pp/8sHBjuzera6xdaDTGfjTn4rJzzfpdGaj0Xgos+JiDSRLzZTKEhrqxMGDbly9Wv3vtqLCQPfu8ncLpmW5/uGv9g+OJrHjqaUW6ybSarUUFBRUbufl5eHt7V3r/vz8/Cr7G+rRRyv4wx8qcHOr+ptRqw28+OI1Wrc226mEMJuQkHK6dNHj6lr97/a110pRqxUKJq6Tm9uYLiAggAMHDlBUVERJSQmZmZn06dOncn/79u1xdXXl66+/BuC9996rsr+hHBxg69YSXnzxGm3b6lGroXv3Ct58s5TJk3VmO48Q5uTqCrt2XSU6ugyN5vrfba9eFaxfX0JEhPlm24l6suMxA5Whpv4aM9m9ezerV69Gp9MxfPhwoqKiiIqKIiYmBj8/P06cOEFsbCxXrlyhe/fuJCQk4OJi/PS523UT3cpWm7iWJllqZi1ZrCUH2G4Wc3YTdexawdmztz+mQwc4c9L2Bvotel1jcHAwwcHBVb6XlJRU+bWvry87duywZAQhhDAfGUAWQgghA8hCCCEsMoCs5EoNN5NiIIQQxjLzALLSKzXczKa7iRwcTKvAph5vSZKlZpKlOmvJAbaZxZyZfdrVfXMbn3bX/5udnU3FLatienp64unpWbl980oNQOVKDdHR0UDNKzUsX76c8PBws7yfm9l0MWjV6jaXa9bAbBeemIFkqZlkqc5acoBk+eIL444rLS1lyJAhXLx4scr3o6OjmTRpUuV2Y6zUYCybLgZCCGGNysrKSEtLq/b9m1sF0DgrNRhLioEQQpjZrd1BtdFqtRw6dKhyu7FXariZDCALIYRClF6p4WYWvQJZCCHE7Vl6pQZjSTEQQggh3URCCCGkGAghhECKgRBCCKQYCCGEQIqBEEIIpBgIIYRAioEQQgjg/wHb5yCe0a9bUAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(-.4,1), .5 - 1*np.linspace(-.4,1), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='x1', ylabel='x2'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)})\n",
    "\n",
    "XOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron tries to find a separating hyperplane for the two response classes. Namely, a set of weights that satisfies:\n",
    "\n",
    "$$\\mathbf{x_1}\\mathbf{w}^T=0$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\mathbf{x_2}\\mathbf{w}^T=0$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{x}_1\\mathbf{w}^T &= \\mathbf{x}_2\\mathbf{w}^T \\\\\n",
    "\\Rightarrow (\\mathbf{x}_1 - \\mathbf{x}_2) \\mathbf{w}^T &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that either the norms of $\\mathbf{x}_1 - \\mathbf{x}_2$ or $\\mathbf{w}$ are zero, or the cosine of the angle between them is equal to zero, due to the identity:\n",
    "\n",
    "$$\\mathbf{a}\\mathbf{b} = \\|a\\| \\|b\\| \\cos \\theta$$\n",
    "\n",
    "Since there is no reason for the norms to be zero in general, we need the two vectors to be at right angles to one another. So, we need a weight vector that is perpendicular to the decision boundary.\n",
    "\n",
    "Clearly, for the XOR function, the output classes are not linearly separable. So, the algorithm does not converge on an answer, but simply cycles through two incorrect solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply **add more weights**.\n",
    "\n",
    "There are two ways to add complexity:\n",
    "\n",
    "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
    "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
    "\n",
    "The latter approach is more common in applications of neural networks.\n",
    "\n",
    "![multilayer](http://d.pr/i/14BS1+)\n",
    "\n",
    "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
    "\n",
    "Updating a multi-layer perceptron (MLP) is a matter of: \n",
    "\n",
    "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
    "2. moving backward updating weights according to the resulting error from forward propagation. \n",
    "\n",
    "In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters.  These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent.  This may be the most common method for training neural networks.  Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors. \n",
    "\n",
    "\n",
    "![bp](https://theclevermachine.files.wordpress.com/2014/09/neural-net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: The chain rule\n",
    "\n",
    "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "In the following derivation, we'll use the following notation:\n",
    "\n",
    "$L$ - Number of layers in the network.\n",
    "\n",
    "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
    "\n",
    "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
    "\n",
    "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
    "\n",
    "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
    "\n",
    "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
    "\n",
    "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
    "\n",
    "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
    "\n",
    "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in general\n",
    "\n",
    "In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\\frac{\\partial C}{\\partial W^m}$ and $\\frac{\\partial C}{\\partial b^m}$.  It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture:\n",
    "\n",
    "- $\\frac{\\partial C}{\\partial a^L}$: The derivative of the cost function with respect to its argument, the output of the network\n",
    "- $\\frac{\\partial a^m}{\\partial z^m}$: The derivative of the nonlinearity used in layer $m$ with respect to its argument\n",
    "\n",
    "To compute the gradient of our cost/error function $C$ to $W^m_{ij}$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a^L$, which is itself a function of the linear mix variables $z^m_k$, which are themselves functions of the weight matrices $W^m$ and biases $b^m$.  With this in mind, we can use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial W^m_{ij}}$$\n",
    "\n",
    "Note that by definition \n",
    "$$\n",
    "z^m_k = \\sum_{l = 1}^{N^m} W^m_{kl} a_l^{m - 1} + b^m_k\n",
    "$$\n",
    "It follows that $\\frac{\\partial z^m_k}{\\partial W^m_{ij}}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any elements in $W^m$ except for those in the $k$<sup>th</sup> row, and we are only considering the entry $W^m_{ij}$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial W^m_{ij}} &= \\frac{\\partial}{\\partial W^m_{ij}}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= a^{m - 1}_j\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_k}{\\partial W^m_{ij}} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "a^{m - 1}_j & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The fact that $\\frac{\\partial C}{\\partial a^m_k}$ is $0$ unless $k = i$ causes the summation above to collapse, giving\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1}_j$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
    "\n",
    "Similarly for the bias variables $b^m$, we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
    "\n",
    "As above, it follows that $\\frac{\\partial z^m_k}{\\partial b^m_i}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any element in $b^m$ except $b^m_k$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= 1\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "1 & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The summation also collapses to give\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "\n",
    "Now, we must compute $\\frac{\\partial C}{\\partial z^m_k}$.  For the final layer ($m = L$), this term is straightforward to compute using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L_k} = \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_k}\n",
    "$$\n",
    "\n",
    "or, in vector form\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\n",
    "$$\n",
    "\n",
    "The first term $\\frac{\\partial C}{\\partial a^L}$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.  Similarly, $\\frac{\\partial a^m}{\\partial z^m}$ (for any layer $m$ includling $L$) is the derivative of the layer's nonlinearity with respect to its argument and will depend on the choice of nonlinearity.  For other layers, we again invoke the chain rule:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^m_k} &= \\frac{\\partial C}{\\partial a^m_k} \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial z^{m + 1}_l}{\\partial a^m_k}\\right)\\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial}{\\partial a^m_k} \\left(\\sum_{h = 1}^{N^m} W^{m + 1}_{lh} a_h^m + b_l^{m + 1}\\right)\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l} W^{m + 1}_{lk}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}W^{m + 1\\top}_{kl} \\frac{\\partial C}{\\partial z^{m + 1}_l}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the last simplification was made because by convention $\\frac{\\partial C}{\\partial z^{m + 1}_l}$ is a column vector, allowing us to write the following vector form:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "\n",
    "Note that we now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters:  First, we compute $\\frac{\\partial C}{\\partial z^L_k}$ based on the choice of cost function and nonlinearity.  Then, we recursively can compute $\\frac{\\partial C}{\\partial z^m}$ layer-by-layer based on the term $\\frac{\\partial C}{\\partial z^{m + 1}}$ computed from the previous layer and the nonlinearity of the layer (this is called the \"backward pass\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "\n",
    "As discussed above, the exact form of the updates depends on both the chosen cost function and each layer's chosen nonlinearity.  The following two table lists the some common choices for nonlinearities and the required partial derivative for deriving the gradient for each layer:\n",
    "\n",
    "| Nonlinearity | $a^m = \\sigma^m(z^m)$ | $\\frac{\\partial a^m}{\\partial z^m}$ | Notes |\n",
    "|--------------|---|---|---|\n",
    "| Sigmoid      | $\\frac{1}{1 + e^{z^m}}$ | $\\sigma^m(z^m)(1 - \\sigma^m(z^m)) = a^m(1 - a^m)$ | \"Squashes\" any input to the range $[0, 1]$ |\n",
    "| Tanh         | $\\frac{e^{z^m} - e^{-z^m}}{e^{z^m} + e^{-z^m}}$ | $1 - (\\sigma^m(z^m))^2 = 1 - (a^m)^2$ | Equivalent, up to scaling, to the sigmoid function |\n",
    "| ReLU         | $\\max(0, z^m)$ | $0, z^m < 0;\\; 1, z^m \\ge 0$ | Commonly used in neural networks with many layers|\n",
    "\n",
    "Similarly, the following table collects some common cost functions and the partial derivative needed to compute the gradient for the final layer:\n",
    "\n",
    "| Cost Function | $C$                                  | $\\frac{\\partial C}{\\partial a^L}$ | Notes |\n",
    "|---------------|--------------------------------------|-----------------------------------|---|\n",
    "| Squared Error | $\\frac{1}{2}(y - a^L)^\\top(y - a^L)$ | $y - a^L$                         | Commonly used when the output is not constrained to a specific range |\n",
    "| Cross-Entropy | $(y - 1)\\log(1 - a^L) - y\\log(a^L)$  | $\\frac{a^L - y}{a^L(1 - a^L)}$    | Commonly used for binary classification tasks; can yield faster convergence |\n",
    "\n",
    "In practice, backpropagation proceeds in the following manner for each training sample:\n",
    "\n",
    "1. Forward pass: Given the network input $a^0$, compute $a^m$ recursively by\n",
    " $$a^1 = \\sigma^1(W^1 a^0 + b^1), \\ldots, a^L = \\sigma^L(W^L a^{L - 1} + b^L)$$\n",
    "1. Backward pass: Compute \n",
    "$$\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}$$\n",
    "for the final layer based on the tables above, then recursively compute\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "for all other layers.  Plug these values into \n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1 \\top}$$\n",
    "and\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "to obtain the updates.\n",
    "\n",
    "### Example: Sigmoid network with cross-entropy loss using gradient descent\n",
    "\n",
    "A common network architecture is one with fully connected layers where each layer's nonlinearity is the sigmoid function $a^m = \\frac{1}{1 + e^{z^m}}$ and the cost function is the cross-entropy loss $(y - 1)\\log(1 - a^L) - y\\log(a^L)$.  To compute the updates for gradient descent, we first compute (based on the tables above)\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^L} &= \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\\\\\n",
    "&= \\left(\\frac{a^L - y}{a^L(1 - a^L)}\\right)a^L(1 - a^L)\\\\\n",
    "&= a^L - y\n",
    "\\end{align*}\n",
    "From here, we can compute\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^{L - 1}} &= \\left(W^{L\\top} \\frac{\\partial C}{\\partial z^L} \\right) \\circ \\frac{\\partial a^{L - 1}}{\\partial z^{L - 1}}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\\\\n",
    "\\frac{\\partial C}{\\partial z^{L - 2}} &= \\left(W^{L - 1\\top} \\frac{\\partial C}{\\partial z^{L - 1}} \\right) \\circ \\frac{\\partial a^{L - 2}}{\\partial z^{L - 2}}\\\\\n",
    "&= W^{L - 1\\top} \\left(W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\right) \\circ a^{L - 2}(1 - a^{L - 2})\n",
    "\\end{align*}\n",
    "and so on, until we have computed $\\frac{\\partial C}{\\partial z^m}$ for $m \\in \\{1, \\ldots, L\\}$.  This allows us to compute  $\\frac{\\partial C}{\\partial W^m_{ij}}$ and $\\frac{\\partial C}{\\partial b^m_i}$, e.g.\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W^L} &= \\frac{\\partial C}{\\partial z^L} a^{L - 1 \\top}\\\\\n",
    "&= (a^L - y)a^{L - 1\\top}\\\\\n",
    "\\frac{\\partial C}{\\partial W^{L - 1}} &= \\frac{\\partial C}{\\partial z^{L - 1}} a^{L - 2 \\top}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1}) a^{L - 2\\top}\n",
    "\\end{align*}\n",
    "and so on.  Standard gradient descent then updates each parameter as follows:\n",
    "$$W^m = W^m - \\lambda \\frac{\\partial C}{\\partial W^m}$$\n",
    "$$b^m = b^m - \\lambda \\frac{\\partial C}{\\partial b^m}$$\n",
    "where $\\lambda$ is the learning rate.  This process is repeated until some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Python example\n",
    "\n",
    "Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations.  The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure python 3 forward compatibility\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = np.random.randn(n_output, n_input)\n",
    "        self.b = np.random.randn(n_output, 1)\n",
    "    def output(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return sigmoid(self.W.dot(X) + self.b)\n",
    "\n",
    "class SigmoidNetwork:\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        '''\n",
    "        :parameters:\n",
    "            - layer_sizes : list of int\n",
    "                List of layer sizes of length L+1 (including the input dimensionality)\n",
    "        '''\n",
    "        self.layers = []\n",
    "        for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(SigmoidLayer(n_input, n_output))\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.2):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(1, -1)\n",
    "        \n",
    "        # Forward pass - compute a^n for n in {0, ... L}\n",
    "        layer_outputs = [X]\n",
    "        for layer in self.layers:\n",
    "            layer_outputs.append(layer.output(layer_outputs[-1]))\n",
    "        \n",
    "        # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1}\n",
    "        cost_partials = [layer_outputs[-1] - y]\n",
    "        for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])):\n",
    "            cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output))\n",
    "        cost_partials.reverse()\n",
    "        \n",
    "        # Compute weight gradient step\n",
    "        W_updates = []\n",
    "        for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]):\n",
    "            W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1])\n",
    "        # and biases\n",
    "        b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]]\n",
    "        \n",
    "        for W_update, b_update, layer in zip(W_updates, b_updates, self.layers):\n",
    "            layer.W -= W_update*learning_rate\n",
    "            layer.b -= b_update*learning_rate\n",
    "\n",
    "    def output(self, X):\n",
    "        a = np.array(X)\n",
    "        if a.ndim == 1:\n",
    "            a = a.reshape(-1, 1)\n",
    "        for layer in self.layers:\n",
    "            a = layer.output(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\tOutput\tQuantized\n",
      "[0, 0]\t0.0128\t[0]\n",
      "[1, 0]\t0.9847\t[1]\n",
      "[0, 1]\t0.9849\t[1]\n",
      "[1, 1]\t0.0246\t[0]\n"
     ]
    }
   ],
   "source": [
    "nn = SigmoidNetwork([2, 2, 1])\n",
    "X = np.array([[0, 1, 0, 1], \n",
    "              [0, 0, 1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "for n in range(int(1e3)):\n",
    "    nn.train(X, y, learning_rate=1.)\n",
    "print(\"Input\\tOutput\\tQuantized\")\n",
    "for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n",
    "    print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] > .5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a56f62ac97349e3a0b5879cca7b1822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='beta', max=25, min=-1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic = lambda h, beta: 1./(1 + np.exp(-beta * h))\n",
    "\n",
    "@interact(beta=(-1, 25))\n",
    "def logistic_plot(beta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    plt.plot(hvals, logistic(hvals, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the advantage of having a simple derivative:\n",
    "\n",
    "$$\\frac{dg}{dh} = \\beta g(h)(1 - g(h))$$\n",
    "\n",
    "Alternatively, the hyperbolic tangent function is also sigmoid:\n",
    "\n",
    "$$g(h) = \\tanh(h) = \\frac{\\exp(h) - \\exp(-h)}{\\exp(h) + \\exp(-h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b603909d6c5b4dafa8161768aa9eb241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='theta', max=25, min=-1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h))\n",
    "\n",
    "@interact(theta=(-1, 25))\n",
    "def tanh_plot(theta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    h = hvals*theta\n",
    "    plt.plot(hvals, hyperbolic_tangent(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "---\n",
    "The simplest algorithm for iterative minimization of differentiable functions is known as just **gradient descent**.\n",
    "Recall that the gradient of a function is defined as the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(x) =  [{\\partial{f}{x_1}, \\partial{f}{x_2}, \\ldots, \\partial{f}{x_n}}]$$\n",
    "\n",
    "and that the gradient of a function always points towards the direction of maximal increase at that point.\n",
    "\n",
    "Equivalently, it points *away* from the direction of maximum decrease - thus, if we start at any point, and keep moving in the direction of the negative gradient, we will eventually reach a local minimum.\n",
    "\n",
    "This simple insight leads to the Gradient Descent algorithm. Outlined algorithmically, it looks like this:\n",
    "\n",
    "1. Pick a point $x_0$ as your initial guess.\n",
    "2. Compute the gradient at your current guess:\n",
    "$v_i = \\nabla f(x_i)$\n",
    "3. Move by $\\alpha$ (your step size) in the direction of that gradient:\n",
    "$x_{i+1} = x_i + \\alpha v_i$\n",
    "4. Repeat steps 1-3 until your function is close enough to zero (until $f(x_i) < \\varepsilon$ for some small tolerance $\\varepsilon$)\n",
    "\n",
    "Note that the step size, $\\alpha$, is simply a parameter of the algorithm and has to be fixed in advance. \n",
    "\n",
    "![gd](http://ludovicarnold.altervista.org/wp-content/uploads/2015/01/gradient-trajectory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the hyperbolic tangent function asymptotes at -1 and 1, rather than 0 and 1, which is sometimes beneficial, and its derivative is simple:\n",
    "\n",
    "$$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Performing gradient descent will allow us to change the weights in the direction that optimially reduces the error. The next trick will be to employ the **chain rule** to decompose how the error changes as a function of the input weights into the change in error as a function of changes in the inputs to the weights, mutliplied by the changes in input values as a function of changes in the weights. \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial h}\\frac{\\partial h}{\\partial w}$$\n",
    "\n",
    "This will allow us to write a function describing the activations of the output weights as a function of the activations of the hidden layer nodes and the output weights, which will allow us to propagate error backwards through the network.\n",
    "\n",
    "The second term in the chain rule simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial h_k}{\\partial w_{jk}} &= \\frac{\\partial \\sum_l w_{lk} a_l}{\\partial w_{jk}}  \\\\\n",
    "&= \\sum_l \\frac{\\partial w_{lk} a_l}{\\partial w_{jk}} \\\\\n",
    "& = a_j\n",
    "\\end{align}$$\n",
    "\n",
    "where $a_j$ is the activation of the jth hidden layer neuron.\n",
    "\n",
    "For the first term in the chain rule above, we decompose it as well:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial h_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k} = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k}$$\n",
    "\n",
    "The second term of this chain rule is just the derivative of the activation function, which we have chosen to have a conveneint form, while the first term simplifies to:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial g(h_k)} = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2} \\sum_k (t_k - y_k)^2 \\right] = t_k - y_k$$\n",
    "\n",
    "Combining these, and assuming (for illustration) a logistic activiation function, we have the gradient:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Which ends up getting plugged into the weight update formula that we saw in the single-layer perceptron:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Note that here we are *subtracting* the second term, rather than adding, since we are doing gradient descent.\n",
    "\n",
    "We can now outline the MLP learning algorithm:\n",
    "\n",
    "1. Initialize all $w_{jk}$ to small random values\n",
    "2. For each input vector, conduct forward propagation:\n",
    "    * compute activation of each neuron $j$ in hidden layer (here, sigmoid):\n",
    "    $$h_j = \\sum_i x_i v_{ij}$$\n",
    "    $$a_j = g(h_j) = \\frac{1}{1 + \\exp(-\\beta h_j)}$$\n",
    "    * when the output layer is reached, calculate outputs similarly:\n",
    "    $$h_k = \\sum_k a_j w_{jk}$$\n",
    "    $$y_k = g(h_k) = \\frac{1}{1 + \\exp(-\\beta h_k)}$$\n",
    "3. Calculate loss for resulting predictions:\n",
    "    * compute error at output:\n",
    "    $$\\delta_k = (t_k - y_k) y_k (1-y_k)$$\n",
    "4. Conduct backpropagation to get partial derivatives of cost with respect to weights, and use these to update weights:\n",
    "    * compute error of the hidden layers:\n",
    "    $$\\delta_{hj} = \\left[\\sum_k w_{jk} \\delta_k \\right] a_j(1-a_j)$$\n",
    "    * update output layer weights:\n",
    "    $$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j$$\n",
    "    * update hidden layer weights:\n",
    "    $$v_{ij} \\leftarrow v_{ij} - \\eta \\delta_{hj} x_i$$\n",
    "    \n",
    "Return to (2) and iterate until learning completes. Best practice is to shuffle input vectors to avoid training in the same order.\n",
    "\n",
    "Its important to be aware that because gradient descent is a hill-climbing (or descending) algorithm, it is liable to be caught in local minima with respect to starting values. Therefore, it is worthwhile training several networks using a range of starting values for the weights, so that you have a better chance of discovering a globally-competitive solution.\n",
    "\n",
    "One useful performance enhancement for the MLP learning algorithm is the addition of **momentum** to the weight updates. This is just a coefficient on the previous weight update that increases the correlation between the current weight and the weight after the next update. This is particularly useful for complex models, where falling into local mimima is an issue; adding momentum will give some weight to the previous direction, making the resulting weights essentially a weighted average of the two directions. Adding momentum, along with a smaller learning rate, usually results in a more stable algorithm with quicker convergence. When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "A weight update with momentum looks like this:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j + \\alpha \\Delta w_{jk}^{t-1}$$\n",
    "\n",
    "where $\\alpha$ is the momentum (regularization) parameter and $\\Delta w_{jk}^{t-1}$ the update from the previous iteration.\n",
    "\n",
    "The multi-layer pereptron is implemented below in the `MLP` class. The implementation uses the scikit-learn interface, so it is uses in the same way as other supervised learning algorithms in that package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
